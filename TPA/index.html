<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>TPA</title>
  <link rel="stylesheet" href="../css/bootstrap.min.css">
  <style>
    body {
      background-image: url("../bg-subject.jpg");
      background-repeat: repeat;
      background-attachment: fixed;
      background-size: 100%;
    }

    .project_name {
      top: 80px;
      font-size: 35px;
      text-align: center;
      color: #eb5424;
      font-family: 'FontAwesome';
      font-weight: bold;
    }

    .project_detail {
      top: 80px;
      font-size: 100px;
    }

    .subvideo {
      border-radius: 0px;
      vertical-align: middle;
      position: relative;
      text-decoration: none;
      font-size: 35px;
      font-weight: 600;
      color: white;
      width: 66%;
      text-align: left;
      font-family: "等线";
      min-height: 36%;
      max-height: 36%;
      background-color: rgba(0, 0, 0, 0.25);
      display: inline-block;
      -webkit-box-shadow: 0px 0px 0px 1px rgba(255, 255, 255, 0.4)inset;
      -moz-box-shadow: 0px 0px 0px 1px rgba(255, 255, 255, 0.4)inset;
      margin: 0 6px 0px 0;
      overflow: hidden;
      line-height: 40px;
    }

    .subvideo:hover {
      margin-top: -3px;
      margin-right: 3px;
      margin-bottom: -3px;
      margin-left: -3px;
      border: 3px solid #000;
    }

    .project_photo {
      top: -0.6%;
      text-align: center;
      padding-top: 0%;
      padding-bottom: 0px;
      border-radius: 20px;
    }

    .describe {
      width: 66%;
      height: 12%;
      margin: auto;
    }

    .describe_detail {
      position: center;
      color: black;
      font-size: 20px;
      text-align: left;
      word-break: normal | break-all | keep-all;
      font-family: 'FontAwesome'
    }

    .github {
      position: fixed;
      top: 17%;
      right: 10%;
      width: 50px;
      height: width;
      border-radius: 100%;
    }

    .github:hover {
      src: url("software.png");
    }

    .software {
      position: fixed;
      top: 24.3%;
      right: 10%;
      width: 50px;
      height: 50px;
      border-radius: 100%;
    }

    .back {
      position: fixed;
      top: 0%;
      left: 0%;
      width: 50px;
      height: 50px;
      border-radius: 100%;
    }

    .last_arrow {}

    .next_arrow {}

    .last_img {
      position: fixed;
      top: 42.5%;
      left: 2.5%;
      width: 12%;
      height: width;
      border-radius: 8px;
    }

    .last_img:hover {
      margin-top: -3px;
      margin-right: -3px;
      margin-bottom: -3px;
      margin-left: -3px;
      border: 3px solid #000;
    }

    .next_img {
      position: fixed;
      top: 42.5%;
      right: 2.5%;
      width: 12%;
      height: width;
      border-radius: 8px;
    }

    .next_img:hover {
      margin-top: -3px;
      margin-right: -3px;
      margin-bottom: -3px;
      margin-left: -3px;
      border: 3px solid #000;
    }

    .up {
      position: fixed;
      top: 18.2%;
      left: 7%;
      width: 3%;
      height: 50px;
      border-radius: 100%;
    }

    .up:hover {
      top: 17.2%;
      margin-top: -3px;
      margin-right: -3px;
      margin-bottom: -3px;
      margin-left: -3px;
      border: 3px solid #000;
    }

    .down {
      position: fixed;
      top: 71.6%;
      left: 7%;
      width: 3%;
      height: 50px;
      border-radius: 100%;
    }

    .down:hover {
      top: 72.6%;
      margin-top: -3px;
      margin-right: -3px;
      margin-bottom: -3px;
      margin-left: -3px;
      border: 3px solid #000;
    }

    .navbar-default {
      border: none;
      -webkit-box-shadow: 0 2px 8px 0 rgba(50, 50, 50, 0.08);
      box-shadow: 0 2px 8px 0 rgba(50, 50, 50, 0.08);
      margin: 0;
      padding: 0;
    }

    .navbar-default .navbar-brand {
      color: #212227;
      font-weight: bold;
      font-size: 20px;
      line-height: 45px;
      padding: 5px 0 0 12px;
      font-family: 'FontAwesome';
    }

    .navbar-default .navbar-nav li a {
      color: #202020;
      font-size: 15px;
      font-weight: bold;
      line-height: 20px;
      letter-spacing: 1px;
      font-family: 'FontAwesome';
    }

    .navbar-default .navbar-nav li a:hover {
      color: #eb5424;
    }

    .navbar-default .navbar-nav>li>a:hover,
    .navbar-default .navbar-nav>li>a:focus {
      color: #eb5424;
      background-color: transparent;
    }

    .navbar-default .navbar-nav>.active>a,
    .navbar-default .navbar-nav>.active>a:hover,
    .navbar-default .navbar-nav>.active>a:focus {
      color: #eb5424;
      background-color: transparent;
    }

    .navbar-default .navbar-toggle {
      border: none;
      padding-top: 10px;
    }

    .navbar-default .navbar-toggle .icon-bar {
      background: #eb5424;
      border-color: transparent;
    }

    .navbar-default .navbar-toggle:hover,
    .navbar-default .navbar-toggle:focus {
      background-color: transparent
    }
  </style>

</head>

<body>
  <div class="navbar navbar-default bs-dos-nav navbar-fixed-top sticky-navigation" role="navigation">
    <div class="container">

      <div class="navbar-header">
        <button class="navbar-toggle" data-toggle="collapse" data-target="#rock-navigation">
          <span class="icon icon-bar"></span>
          <span class="icon icon-bar"></span>
          <span class="icon icon-bar"></span>
        </button>
        <a href="https://lauyikfung.github.io" class="navbar-brand">Lewis Yik-fung Lau</a>
      </div>
      <nav class="collapse navbar-collapse" id="rock-navigation">
        <ul class="nav navbar-nav navbar-right main-navigation text-uppercase">
          <li><a href="https://lauyikfung.github.io/MARS" class="smoothScroll">Next Project</a></li>
          <li><a href="https://arxiv.org/abs/2501.06425" class="smoothScroll">Paper</a></li>
          <li><a href="https://github.com/tensorgi/T6" class="smoothScroll">Github</a></li>
          <li><a href="https://arxiv.org/pdf/2501.06425" class="smoothScroll">PDF</a></li>
        </ul>
      </nav>

    </div>
  </div>
  <div class="project_name">
    <p class="name_detail">Tensor Product Attention Is All You Need</p>
  </div>
  <div class="project_name">
    <p class="name_detail">Tensor Product Attention Is All You Need</p>
  </div>
  <div class="project_photo">
    <img src="../TPA.png" class="subvideo project_photo"></img>
  </div>
  <div class="describe">
    <p class="describe_detail" align="center">
      缩放语言模型以处理较长的输入序列通常需要大的键值（KV）缓存，从而在推理过程中产生大量的内存开销。
      在本文中，我们提出了张量乘积注意（<b>TPA</b> ），这是一种新的注意机制，它使用张量分解来紧凑地表示查询、键和值，从而在推理时显著缩小键值缓存大小。
      通过将这些表示分解为上下文低秩组件（上下文分解）并与RoPE无缝集成，TPA在提高内存效率的同时提高了模型质量。
      基于TPA，我们介绍了Tensor ProducT ATTenTion Transformer（T6），这是一种新的序列建模模型架构。通过对语言建模任务的广泛实证评估，我们证明T6在各种指标上超过了包括MHA、MQA、GQA和MLA等基线的性能，包括困惑度等一系列评估基准。
      值得注意的是，TPA的内存效率使得在固定资源限制下模型鞥能够处理更长的序列，解决了现代语言模型中的关键可扩展性挑战。<br>
    <ul class="describe_detail">
      <li>共同一作：张伊凡（清华大学交叉信息研究院、上海期智研究院），刘益枫（UCLA计算机系）<br></li>
      <li>通讯作者：姚期智院士（清华大学交叉信息研究院、上海期智研究院）<br></li>
      <li>其他作者：袁会卓（UCLA校友）、Zhen Qin（TapTap）、袁洋教授（清华大学交叉信息研究院、上海期智研究院）、顾全全教授（UCLA计算机系）</li>
    </ul>
    </p>
    <p class="describe_detail">
      Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. 
      In this paper, we propose Tensor Product Attention (<b>TPA</b> ), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. 
      By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. 
      Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. 
      Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models.<br>
    <ul class="describe_detail">
      <li>Co-first authors: Yifan Zhang (IIIS, Tsinghua & Shanghai Qi Zhi Institute), Yifeng Liu (UCLA CS Dept.)</li>
      <li>Corresponding authors: Academician Andrew Chi-Chih Yao (IIIS, Tsinghua & Shanghai Qi Zhi Institute)</li>
      <li>Other authors: Huizhuo Yuan (Alumna of UCLA), Zhen Qin (TapTap), Prof. Yang Yuan (IIIS, Tsinghua & Shanghai Qi Zhi Institute), Prof. Quanquan Gu (UCLA CS Dept.)</li>
      </li>
    </ul>
    </p>
  </div>
  <a href="https://lauyikfung.github.io/MARS">
    <img src="../MARS.png" class="next_img"></img></a>
</body>

</html>